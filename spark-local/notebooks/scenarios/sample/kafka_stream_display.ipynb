{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0938f1-169b-4a95-930a-ae0010318d8a",
   "metadata": {},
   "source": [
    "# Stream test  \n",
    "hdfs 에 구매 요청 정보가 들어오는 경우, stream 방식으로 요청을 수신하고 처리  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73efbbd2-73ff-4cbf-a6db-ad2793fe4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 임포트  \n",
    "import socket\n",
    "import sys\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from os.path import abspath\n",
    "import findspark\n",
    "import time \n",
    "\n",
    "# 환경변수 정의  \n",
    "scale = 1000 # 1000 만 건 수준\n",
    "APP_NAME = 'stream-kafka'\n",
    "\n",
    "\n",
    "# 스파크 생성 \n",
    "def spark_creation_yarn():\n",
    "    spark = SparkSession.builder.master('yarn').appName(APP_NAME)\\\n",
    "    .config('spark.driver.cores', '2').config('spark.driver.memory', '4g')\\\n",
    "    .config('spark.num.executors', '2')\\\n",
    "    .config('spark.executor.cores', '2').config('spark.executor.memory', '4g')\\\n",
    "    .config('spark.jars', '/hive-bin/lib/mysql-connector-java-5.1.49-bin.jar')\\\n",
    "    .config('spark.driver.extraClassPath', '/hive-bin/lib/mysql-connector-java-5.1.49-bin.jar')\\\n",
    "    .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    return spark\n",
    "\n",
    "def spark_creation_local():\n",
    "    kafka_spark_jar_path = \"/spark-git/work-k/jar/spark-streaming-kafka-0-10_2.12-3.2.1.jar,/spark-git/work-k/jar/spark-sql-kafka-0-10_2.12-3.2.1.jar,/spark-git/work-k/jar/kafka-clients-0.10.0.1.jar\"\n",
    "    spark = SparkSession.builder.master('local[*]').appName(APP_NAME)\\\n",
    "    .config(\"spark.jars\", kafka_spark_jar_path)\\\n",
    "    .config(\"spark.driver.extraClassPath\", kafka_spark_jar_path)\\\n",
    "    .getOrCreate()\n",
    "# .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12-3.2.1\")\\\n",
    "    #    .config(\"spark.driver.extraClassPath\", kafka_spark_jar_path)\\\n",
    "    #     .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord) \\\n",
    "    # .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord) \\\n",
    "    sc = spark.sparkContext\n",
    "    # spark.conf.set(\"spark.driver.extraClassPath\", kafka_spark_jar_path)\n",
    "    return spark\n",
    "def spark_creation():\n",
    "    return spark_creation_local()\n",
    "\n",
    "def display_output(outTarget, sqlString, interval = 1):\n",
    "    from IPython.display import display, clear_output\n",
    "    from pyspark.sql.streaming import StreamingQuery \n",
    "    import time \n",
    "    if isinstance(outTarget, StreamingQuery):\n",
    "        while True:\n",
    "            clear_output(wait=False)\n",
    "            print(f\"Query : {sqlString}\")\n",
    "            # display(spark.sql(sqlString).show())\n",
    "            print(f\"outTarget : {outTarget}\")\n",
    "            # print(f\"temp : {(temp[0])}\")\n",
    "            #display(outTarget.select(\"type\").show())\n",
    "            time.sleep(interval)\n",
    "    else:\n",
    "        print(\"Not instance......\")\n",
    "        spark.sql(sqlString).show()\n",
    "        \n",
    "def define_schema():\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "    columns = [\n",
    "        StructField(\"type\", StringType())\n",
    "        , StructField(\"qty\", LongType())\n",
    "    ]\n",
    "    inven_schema = StructType(columns)\n",
    "    return inven_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b7de39-bd37-455b-85c7-e694410364d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 ms, sys: 46 ms, total: 69.9 ms\n",
      "Wall time: 6.47 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.28.35.245:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>stream-kafka</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4267ec2a20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark = spark_creation()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42990cd8-a0d7-4df8-85e5-8b3638dc1a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82136ad-246a-4798-a7f3-ba90e4852cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to 1 topic\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"quickstart-events\") \\\n",
    "  .load()\n",
    "\n",
    "# df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6112b44b-b264-47f5-bfee-afd5368480f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718a592-37f5-469b-b8df-29dac9d4156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.createOrReplaceTempView(\"mytable\")\n",
    "query_name = \"lines_12\"\n",
    "# lines.createOrReplaceTempView(query_name)\n",
    "sql = f\"select * from mytable \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a2dd1f-b468-49b4-9e53-0c09eb05c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activityQuery = df.writeStream.queryName(query_name).format(\"memory\").outputMode(\"append\").start()\n",
    "# activityQuery.awaitTermination() \n",
    "# spark.streams.active\n",
    "\n",
    "# from time import sleep\n",
    "# for x in range(3):\n",
    "#     spark.sql(sql).show(3)\n",
    "#     sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ed9c13-b08c-4352-bec5-1f6510563d19",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "org/apache/spark/kafka010/KafkaConfigUpdater\n=== Streaming Query ===\nIdentifier: mytable [id = 40a1c68e-3baa-44e6-baed-20d601545080, runId = 6b308e1d-b9df-4fd1-9ab6-63bc50384942]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {}\n\nCurrent State: INITIALIZING\nThread State: RUNNABLE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-10256072d7e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moutQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: org/apache/spark/kafka010/KafkaConfigUpdater\n=== Streaming Query ===\nIdentifier: mytable [id = 40a1c68e-3baa-44e6-baed-20d601545080, runId = 6b308e1d-b9df-4fd1-9ab6-63bc50384942]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {}\n\nCurrent State: INITIALIZING\nThread State: RUNNABLE"
     ]
    }
   ],
   "source": [
    "outQ = df \\\n",
    "    .withWatermark(\"timestamp\", \"5 seconds\")\\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"mytable\") \\\n",
    "    .start()\n",
    "\n",
    "outQ.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27048a-ed76-4ac3-93e2-faf1580b27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import time  \n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(outQ.status)\n",
    "    display(outQ.lastProgress)\n",
    "    display(spark.sql(f'SELECT * FROM mytable').show())\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00b15031-c2ce-4223-88af-69a0daaa5e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Terminated with exception: org/apache/spark/kafka010/KafkaConfigUpdater',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outQ.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba11ac1-d1e5-41a0-a522-4f284345421c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7b2f0b-dbb2-4511-af94-9e7ba6c68faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
