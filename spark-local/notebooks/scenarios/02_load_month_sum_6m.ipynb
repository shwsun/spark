{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01985d71-d0ba-46fc-ae43-2b361adc5308",
   "metadata": {},
   "source": [
    "# 일반 구매 차감 \n",
    "1000만 세톱 데이터에 대해 일반 구매 차감 연산 실행한 결과  \n",
    " - 전체 : 94 초  \n",
    " - 세션 생성 : 23 초  \n",
    " - 차감 계산 : 37 초 (14초.캐시)    \n",
    " - 스냅샷 저장 : 32 초  \n",
    " - 결과 확인 : 2 초  \n",
    " - 요약 결과 생성 : \n",
    " - 요약 rdb 기록 : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0657f239-2675-409d-820d-2ab11bb449d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 임포트  \n",
    "import socket\n",
    "import sys\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from os.path import abspath\n",
    "import findspark\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c16fd2f-27cc-4493-83b7-53d4762537f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 정의  \n",
    "scale = 1000 # 1000 만 건 수준\n",
    "PRJ_ROOT = '/user/root'\n",
    "APP_NAME = f'spark-02-load-inven-6m'\n",
    "DB_NAME = 'inven'\n",
    "\n",
    "# 데이터의 파일 포맷 및 파일명  \n",
    "tbl_setop_name = f'inven/table-set-6m-50-1000'\n",
    "file_format = 'parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d6d1dd4-e8d4-4a9b-a8d7-3c132f54f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 생성 \n",
    "def spark_creation():\n",
    "    spark = SparkSession.builder.master('yarn').appName(APP_NAME)\\\n",
    "    .config('spark.driver.cores', '2').config('spark.driver.memory', '4g')\\\n",
    "    .config('spark.num.executors', '4')\\\n",
    "    .config('spark.executor.cores', '2').config('spark.executor.memory', '2g')\\\n",
    "    .config('spark.jars', '/hive-bin/lib/mysql-connector-java-5.1.49-bin.jar')\\\n",
    "    .config('spark.driver.extraClassPath', '/hive-bin/lib/mysql-connector-java-5.1.49-bin.jar').getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    sc\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79911c4d-b827-40f8-9ccd-cb1180d3bb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root   1006906 Mar 10 06:57 mysql-connector-java-5.1.49-bin.jar\n",
      "-rw-r--r-- 1 root staff    10476 Nov 15  2018 mysql-metadata-storage-0.12.0.jar\n"
     ]
    }
   ],
   "source": [
    "!ls -l /hive-bin/lib | grep mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd0b5ee-33b2-4c03-8fde-16c4a011adbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.4 ms, sys: 8.56 ms, total: 59 ms\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = spark_creation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8dfe5a-5097-4cf9-9670-e019e7e124c6",
   "metadata": {},
   "source": [
    "## 전체 구매 차감  \n",
    "모든 세탑에 (지정 비율 * 구매 단위) 값을 차감  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe1b1723-4f5d-4232-9e86-01ee9f639dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------------------+\n",
      "|     cnt|sum(((inv_req_01 + inv_req_02) + inv_req_03))|\n",
      "+--------+---------------------------------------------+\n",
      "|10000000|                                            0|\n",
      "+--------+---------------------------------------------+\n",
      "\n",
      "+--------+---------------------------------------------+\n",
      "|     cnt|sum(((inv_req_01 + inv_req_02) + inv_req_03))|\n",
      "+--------+---------------------------------------------+\n",
      "|10000000|                                       -3.0E8|\n",
      "+--------+---------------------------------------------+\n",
      "\n",
      "CPU times: user 16 ms, sys: 5.93 ms, total: 21.9 ms\n",
      "Wall time: 35.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# sparkSQL로 전체 데이터 읽어서 카운트  \n",
    "# 건 수 차이 없는 방식. 최초, 재실행 여부 차이.  \n",
    "# 최초 : 22 초 , 재실행 : 13 초   \n",
    "## 구매 요청 값 \n",
    "req_val = -10 \n",
    "## 인벤 기준 정보 조회  \n",
    "spark.read.format(file_format).load(tbl_setop_name).createOrReplaceTempView('setop_view')\n",
    "## 요청 구매 단위 * 비율 계산  \n",
    "sql_req_rate = f\"select setop, inv_rate_01*{req_val} inv_req_01, inv_rate_02*{req_val} inv_req_02, inv_rate_03*{req_val} inv_req_03\\\n",
    ", inv_rate_04*{req_val} inv_req_04, inv_rate_05*{req_val} inv_req_05, inv_rate_06*{req_val} inv_req_06 from setop_view\"\n",
    "spark.sql(sql_req_rate).createOrReplaceTempView('setop_req_rate')\n",
    "\n",
    "# 차감 요청 정보를 원본에 반영  \n",
    "sql_calc = \"\"\"\n",
    "select setop\n",
    ", sum(inv_req_01) inv_req_01 , sum(inv_req_02) inv_req_02\n",
    ", sum(inv_req_03) inv_req_03 , sum(inv_req_04) inv_req_04\n",
    ", sum(inv_req_05) inv_req_05 , sum(inv_req_06) inv_req_06\n",
    "from \n",
    "(select setop\n",
    ", inv_req_01, inv_req_02, inv_req_03\n",
    ", inv_req_04, inv_req_05, inv_req_06\n",
    "from setop_view \n",
    "UNION ALL \n",
    "select setop\n",
    ", inv_req_01, inv_req_02, inv_req_03\n",
    ", inv_req_04, inv_req_05, inv_req_06\n",
    "from setop_req_rate\n",
    ") as A \n",
    "group by setop \n",
    "\"\"\"\n",
    "spark.sql(sql_calc).createOrReplaceTempView('result')\n",
    "\n",
    "# 계산 결과와 원본 비교  \n",
    "spark.sql(\"select count(1) cnt, sum(inv_req_01+ inv_req_02+ inv_req_03) from setop_view\").show()\n",
    "spark.sql(\"select count(1) cnt, sum(inv_req_01+ inv_req_02+ inv_req_03) from result\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fcd912c-e5e0-41cb-a55b-d2209548a463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 ms, sys: 629 µs, total: 12.1 ms\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 차감한 모든 결과를 result table로 스냅샷 기록  \n",
    "result = spark.sql(\"select * from result\")\n",
    "result.write.save(path=\"result\", format=file_format, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1635bd75-b631-4b7e-9a34-59ebc204ee21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count = 10,000,000\n",
      "CPU times: user 3.71 ms, sys: 0 ns, total: 3.71 ms\n",
      "Wall time: 1.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cnt = spark.read.format(file_format).load('result').count()\n",
    "print(f'Row count = {cnt:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b3044-54e0-43ba-8e3f-257406d051e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdb 기록할 요약 결과 생성 : 채널/시간/주중, 지역별, 주제별   \n",
    "# 우선 hdfs 에 먼저 기록 ; 비율, inv값, 청약량  \n",
    "# 지역별/시간별/주말 ...... \n",
    "# 주제별 요약 : 여러 주제에 할당 되기 때문에, 실제 시간의 몇 배가 나올 듯...  차감하지 않은 주제가 차감되는 현상도 발생한다.   \n",
    "# 비율, inv 값은 원본 테이블의 값. 청약량만 계산 결과.  \n",
    "# -> 일반 구매시 : 채널, 지역, 주제 테이블 차감.  \n",
    "# -> 주제 구매시 : ...\n",
    "# -> 2 경우 모두, setop table에서 차감하고 결과를 요약으로 생성하는 방법만 다른 듯... \n",
    "## 1. 주제 구매시 채널/시간/주중 차감은 비율 적용해서 ... \n",
    "##   - 지역 차감은 ? 지역은 세톱에 영향 받는 데... 주제 세탑 조회해서 여기에 다시 지역 비율 반영해야 할 듯... \n",
    "##   - 주제 차감은 세톱에서? \n",
    "## 2. 시간 구매시 지역/주제 차감은... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27a1ddba-ee81-4281-9dc8-cf1483a36ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------------------------------------------------------------------+\n",
      "|     cnt|sum((((((inv_req_01 + inv_req_02) + inv_req_03) + inv_req_04) + inv_req_05) + inv_req_06))|\n",
      "+--------+------------------------------------------------------------------------------------------+\n",
      "|10000000|                                                                                    -1.2E9|\n",
      "+--------+------------------------------------------------------------------------------------------+\n",
      "\n",
      "+--------+------------------------------------------------------------------------------------------+\n",
      "|     cnt|sum((((((inv_req_01 + inv_req_02) + inv_req_03) + inv_req_04) + inv_req_05) + inv_req_06))|\n",
      "+--------+------------------------------------------------------------------------------------------+\n",
      "|10000000|                                                                                    -1.8E9|\n",
      "+--------+------------------------------------------------------------------------------------------+\n",
      "\n",
      "CPU times: user 23.2 ms, sys: 1.33 ms, total: 24.5 ms\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# 재실행해도 계산 누적되서 빨라지지 않는다.  \n",
    "# spark.sql(\"select * from result\").createOrReplaceTempView('setop_view')\n",
    "# # 재실행 1회 : 14 초 .  5회 : \n",
    "# spark.sql(sql_calc).createOrReplaceTempView('result')\n",
    "\n",
    "# # 계산 결과와 원본 비교  \n",
    "# spark.sql(\"select count(1) cnt, sum(inv_req_01+inv_req_02+inv_req_03+inv_req_04+inv_req_05+inv_req_06) from setop_view\").show()\n",
    "# spark.sql(\"select count(1) cnt, sum(inv_req_01+inv_req_02+inv_req_03+inv_req_04+inv_req_05+inv_req_06) from result\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43c50d60-0ba7-4b5e-8b1e-d0ef26bbabc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mysql test_jdbc db 생성 \n",
    "# set global validate_password_policy=LOW;\n",
    "# set global validate_password_length=3;\n",
    "# CREATE USER 'jdbc'@'%' IDENTIFIED BY 'jdbc';\n",
    "# CREATE DATABASE test_jdbc;\n",
    "# GRANT ALL privileges on test_jdbc.* to 'jdbc'@'%' with GRANT option;\n",
    "# flush privileges;\n",
    "\n",
    "#### \n",
    "# conf = pyspark.SparkConf().setAll([('spark.executor.id', 'driver'), \n",
    "#                                # ('spark.app.id', 'local-1631738601802'), \n",
    "#                                # ('spark.app.name', 'PySparkShell'), \n",
    "#                                # ('spark.driver.port', '32877'), \n",
    "#                                # ('spark.sql.warehouse.dir', 'file:/home/data_analysis_tool/spark-warehouse'), \n",
    "#                                # ('spark.driver.host', 'localhost'), \n",
    "#                                # ('spark.sql.catalogImplementation', 'hive'), \n",
    "#                                # ('spark.rdd.compress', 'True'), \n",
    "#                                # ('spark.driver.bindAddress', 'localhost'), \n",
    "#                                # ('spark.serializer.objectStreamReset', '100'), \n",
    "#                                # ('spark.master', 'local[*]'), \n",
    "#                                # ('spark.submit.pyFiles', ''), \n",
    "#                                # ('spark.app.startTime', '1631738600836'), \n",
    "#                                # ('spark.submit.deployMode', 'client'), \n",
    "#                                # ('spark.ui.showConsoleProgress', 'true'),\n",
    "#                                # ('spark.driver.extraClassPath','/tmp/postgresql-42.2.23.jar')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab6d5521-e13a-4cd7-818d-4810cab4cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# rdb 쓰기 테스트 \n",
    "# conf = SparkConf()  # create the configuration\n",
    "#conf.set(\"spark.jars\", \"/path/to/postgresql-connector-java-someversion-bin.jar\")  # set the spark.jars\n",
    "# --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar\n",
    "# /hadoop/share/hadoop/yarn/lib 에 mysql jdbc.jar 넣는다.  \n",
    "props = {\"driver\":\"com.mysql.jdbc.Driver\"}\n",
    "db_url = \"jdbc:mysql://rdb/test_jdbc?user=jdbc&password=jdbc\"\n",
    "tbl = \"from_spark\"\n",
    "rdb = spark.sql('select * from result limit 10')\n",
    "rdb.write.jdbc(db_url, tbl, mode='overwrite', properties=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6377b62-b427-4143-894e-044bcf81be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jdbcDF = spark.read \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "#     .option(\"dbtable\", \"jdbc:postgresql:dbserver\") \\\n",
    "#     .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8388b65d-c6a4-42b9-aef7-42832525b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
