{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0938f1-169b-4a95-930a-ae0010318d8a",
   "metadata": {},
   "source": [
    "# Stream test  \n",
    "hdfs 에 구매 요청 정보가 들어오는 경우, stream 방식으로 요청을 수신하고 처리  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73efbbd2-73ff-4cbf-a6db-ad2793fe4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 임포트  \n",
    "import socket\n",
    "import sys\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from os.path import abspath\n",
    "import findspark\n",
    "import time \n",
    "\n",
    "# 환경변수 정의  \n",
    "scale = 1000 # 1000 만 건 수준\n",
    "PRJ_ROOT = '/user/root'\n",
    "APP_NAME = '04-stream'\n",
    "DB_NAME = 'inven'\n",
    "\n",
    "# 데이터의 파일 포맷 및 파일명  \n",
    "tbl_name = 'inven/request'\n",
    "file_format = 'csv'\n",
    "\n",
    "# 스파크 생성 \n",
    "def spark_creation():\n",
    "    spark = SparkSession.builder.master('yarn').appName(APP_NAME)\\\n",
    "    .config('spark.driver.cores', '2').config('spark.driver.memory', '4g')\\\n",
    "    .config('spark.num.executors', '2')\\\n",
    "    .config('spark.executor.cores', '2').config('spark.executor.memory', '4g')\\\n",
    "    .config('spark.jars', '/hive-bin/lib/mysql-connector-java-5.1.49-bin.jar')\\\n",
    "    .config('spark.driver.extraClassPath', '/hive-bin/lib/mysql-connector-java-5.1.49-bin.jar').getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    sc\n",
    "    return spark\n",
    "\n",
    "def display_output(outTarget, sqlString, interval = 1):\n",
    "    from IPython.display import display, clear_output\n",
    "    from pyspark.sql.streaming import StreamingQuery \n",
    "    import time \n",
    "    if isinstance(outTarget, StreamingQuery):\n",
    "        while True:\n",
    "            clear_output(wait=False)\n",
    "            #print(f\"Query : {sqlString}\")\n",
    "            display(spark.sql(sqlString).show())\n",
    "            print(f\"outTarget : {outTarget}\")\n",
    "            # print(f\"temp : {(temp[0])}\")\n",
    "            #display(outTarget.select(\"type\").show())\n",
    "            time.sleep(interval)\n",
    "    else:\n",
    "        print(\"Not instance......\")\n",
    "        spark.sql(sqlString).show()\n",
    "        \n",
    "def define_schema():\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "    columns = [\n",
    "        StructField(\"type\", StringType())\n",
    "        , StructField(\"qty\", LongType())\n",
    "    ]\n",
    "    inven_schema = StructType(columns)\n",
    "    return inven_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62b7de39-bd37-455b-85c7-e694410364d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 ms, sys: 2.17 ms, total: 22.6 ms\n",
      "Wall time: 15.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = spark_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6a2dd1f-b468-49b4-9e53-0c09eb05c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|type|qty|\n",
      "+----+---+\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream 요청 처리  \n",
    "from pyspark.sql.functions import explode, split\n",
    "temp = ['']\n",
    "dataSchema = define_schema() \n",
    "#lines = spark.readStream.format(\"hdfs\").option(\"\", \"\").load()\n",
    "lines = spark.readStream.schema(dataSchema).csv(f\"{PRJ_ROOT}/{tbl_name}\")\n",
    "\n",
    "lines.writeStream.option(\"checkpointLocation\", f\"{PRJ_ROOT}/{tbl_name}\").toTable(\"myTable\")\n",
    "spark.read.table(\"myTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba11ac1-d1e5-41a0-a522-4f284345421c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|type|qty|\n",
      "+----+---+\n",
      "+----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outTarget : <pyspark.sql.streaming.StreamingQuery object at 0x7fa3c91ea518>\n"
     ]
    }
   ],
   "source": [
    "query_name = \"lines9\"\n",
    "# lines.createOrReplaceTempView(query_name)\n",
    "sql = f\"select * from myTable \"\n",
    "# df = spark.sql(sql)\n",
    "#outQ = df.writeStream.queryName(query_name).format(\"memory\").trigger(processingTime=\"30 seconds\").outputMode(\"append\").start()\n",
    "outQ = lines.writeStream.queryName(query_name).trigger(processingTime=\"5 seconds\").outputMode(\"append\").format(\"memory\").start()  \n",
    "\n",
    "\n",
    "display_output(outQ, sql)\n",
    "# display_output(lines, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf7b2f0b-dbb2-4511-af94-9e7ba6c68faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
