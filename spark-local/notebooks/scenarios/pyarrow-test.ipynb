{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f0c0d16-5a13-431a-8bad-2ecb296b9e58",
   "metadata": {
    "tags": []
   },
   "source": [
    "# pyarrow 방식 성능 테스트  \n",
    "- cache 누적 처리 시 성능 저하 문제 회피  \n",
    "- 메모리 적재 형태 데이터를 spark로 전달하고 처리 후 다시 pandas 형태로 처리하는 테스트   \n",
    "- hdfs data는 미리 접근 가능한 위치로 옮기고 처리한다 가정...  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a58f38ae-76fd-4436-86a9-190f36e6edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 임포트  \n",
    "import socket\n",
    "import sys\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from os.path import abspath\n",
    "# import findspark\n",
    "import time \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pyarrow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3fc1f09a-d8db-41c0-b3b7-c14ded24c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b221296-467f-4577-895e-adc04c1381ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd958b8-8c7a-45d9-b83a-27c8f2d69923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sys.version\n",
    "# # pd.__version__\n",
    "# import pyspark\n",
    "# pyspark.__version__\n",
    "# # !pip install pandas==1.3.0\n",
    "# pyarrow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d62cca-1e71-4162-af99-1c6a0f80b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 정의  \n",
    "scale = 1000 # 1000 만 건 수준\n",
    "PRJ_ROOT = '/user/root'\n",
    "APP_NAME = 'RDD-Pandas'\n",
    "DB_NAME = 'inven'\n",
    "\n",
    "# 데이터의 파일 포맷 및 파일명  \n",
    "tbl_setop_name = 'inven/table-set-6m-20-1000'\n",
    "file_format = 'parquet' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea1a67a-8ec9-447c-a575-fe53411cce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 생성 \n",
    "def spark_creation():\n",
    "    spark = SparkSession.builder.master('yarn').appName(APP_NAME)\\\n",
    "    .config('spark.rpc.message.maxSize', '2046')\\\n",
    "    .config('spark.driver.maxResultSize', '3000000000')\\\n",
    "    .config('spark.sql.execution.arrow.enabled', 'true')\\\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\\\n",
    "    .config('spark.sql.execution.arrow.pyspark.fallback.enabled', 'true')\\\n",
    "    .config('spark.driver.cores', '1').config('spark.driver.memory', '7g')\\\n",
    "    .config('spark.num.executors', '3')\\\n",
    "    .config('spark.executor.cores', '1').config('spark.executor.memory', '7g')\\\n",
    "    .config('spark.jars', '/hive-bin/lib/mysql-connector-java-5.1.49-bin.jar')\\\n",
    "    .config('spark.driver.extraClassPath', '/hive-bin/lib/mysql-connector-java-5.1.49-bin.jar').getOrCreate()\n",
    "    #     .config('spark.sql.execution.arrow.enabled', 'true')\\\n",
    "    # spark.rpc.message.maxSize  240007497 \n",
    "    sc = spark.sparkContext\n",
    "    sc\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed042fa3-249f-46fb-96a0-59f4b04114f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.2 ms, sys: 19.7 ms, total: 51.9 ms\n",
      "Wall time: 20.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RDD-Pandas</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f31bab21080>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark = spark_creation()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f80cc22a-4d7a-4294-85cb-9d4f9b4ba8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow==7.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb2be4-1021-4956-b217-d69103c8fb68",
   "metadata": {},
   "source": [
    "# 파일 직접 읽기  \n",
    "parq로 저장된 데이터를 직접 읽어서 pandas & arrow로 처리하기  \n",
    "hdfs 경로에 직접 접근 못하므로, hdfs 에서 fs로 파일 미리 옯겨 놓는다고 가정  \n",
    "> pdf 1천만 건을 직접 spark df 변환시에는 mem 부족으로 에러 발생.  \n",
    "> 나눠서는 변환 가능  \n",
    "parition key 지정해서 400만 건 이하로 나누고, 읽을 때나 쓸 때 나눠서 처리하면 사용 가능할 듯   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f01110-7d9b-435d-b587-fbfa55cef3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000000, 20)\n",
      "CPU times: user 3.23 s, sys: 1.85 s, total: 5.08 s\n",
      "Wall time: 3.87 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setop</th>\n",
       "      <th>stype</th>\n",
       "      <th>inv_rate_01</th>\n",
       "      <th>inv_val_01</th>\n",
       "      <th>inv_req_01</th>\n",
       "      <th>inv_rate_02</th>\n",
       "      <th>inv_val_02</th>\n",
       "      <th>inv_req_02</th>\n",
       "      <th>inv_rate_03</th>\n",
       "      <th>inv_val_03</th>\n",
       "      <th>inv_req_03</th>\n",
       "      <th>inv_rate_04</th>\n",
       "      <th>inv_val_04</th>\n",
       "      <th>inv_req_04</th>\n",
       "      <th>inv_rate_05</th>\n",
       "      <th>inv_val_05</th>\n",
       "      <th>inv_req_05</th>\n",
       "      <th>inv_rate_06</th>\n",
       "      <th>inv_val_06</th>\n",
       "      <th>inv_req_06</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ST_B_0499136</td>\n",
       "      <td>ST_B</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ST_B_0499137</td>\n",
       "      <td>ST_B</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ST_B_0499138</td>\n",
       "      <td>ST_B</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ST_B_0499139</td>\n",
       "      <td>ST_B</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ST_B_0499140</td>\n",
       "      <td>ST_B</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          setop stype  inv_rate_01  inv_val_01  inv_req_01  inv_rate_02  \\\n",
       "0  ST_B_0499136  ST_B      0.00001        1000           0      0.00001   \n",
       "1  ST_B_0499137  ST_B      0.00001        1000           0      0.00001   \n",
       "2  ST_B_0499138  ST_B      0.00001        1000           0      0.00001   \n",
       "3  ST_B_0499139  ST_B      0.00001        1000           0      0.00001   \n",
       "4  ST_B_0499140  ST_B      0.00001        1000           0      0.00001   \n",
       "\n",
       "   inv_val_02  inv_req_02  inv_rate_03  inv_val_03  inv_req_03  inv_rate_04  \\\n",
       "0        1000           0      0.00001        1000           0      0.00001   \n",
       "1        1000           0      0.00001        1000           0      0.00001   \n",
       "2        1000           0      0.00001        1000           0      0.00001   \n",
       "3        1000           0      0.00001        1000           0      0.00001   \n",
       "4        1000           0      0.00001        1000           0      0.00001   \n",
       "\n",
       "   inv_val_04  inv_req_04  inv_rate_05  inv_val_05  inv_req_05  inv_rate_06  \\\n",
       "0        1000           0      0.00001        1000           0      0.00001   \n",
       "1        1000           0      0.00001        1000           0      0.00001   \n",
       "2        1000           0      0.00001        1000           0      0.00001   \n",
       "3        1000           0      0.00001        1000           0      0.00001   \n",
       "4        1000           0      0.00001        1000           0      0.00001   \n",
       "\n",
       "   inv_val_06  inv_req_06  \n",
       "0        1000           0  \n",
       "1        1000           0  \n",
       "2        1000           0  \n",
       "3        1000           0  \n",
       "4        1000           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# df = pyarrow.parquet.read_table(source=\"out_parq\").to_pandas()\n",
    "# df\n",
    "import pyarrow.parquet as pq\n",
    "adf = pq.read_table(\"parq\")\n",
    "print(adf.shape)\n",
    "pdf = adf.to_pandas()\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33bdd639-bc10-4fd2-9ab9-091c4bf8500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----------+----------+----------+-----------+----------+----------+-----------+----------+----------+-----------+----------+----------+-----------+----------+----------+-----------+----------+----------+\n",
      "|       setop|stype|inv_rate_01|inv_val_01|inv_req_01|inv_rate_02|inv_val_02|inv_req_02|inv_rate_03|inv_val_03|inv_req_03|inv_rate_04|inv_val_04|inv_req_04|inv_rate_05|inv_val_05|inv_req_05|inv_rate_06|inv_val_06|inv_req_06|\n",
      "+------------+-----+-----------+----------+----------+-----------+----------+----------+-----------+----------+----------+-----------+----------+----------+-----------+----------+----------+-----------+----------+----------+\n",
      "|ST_G_0997120| ST_G|     1.0E-5|      1000|         0|     1.0E-5|      1000|         0|     1.0E-5|      1000|         0|     1.0E-5|      1000|         0|     1.0E-5|      1000|         0|     1.0E-5|      1000|         0|\n",
      "+------------+-----+-----------+----------+----------+-----------+----------+----------+-----------+----------+----------+-----------+----------+----------+-----------+----------+----------+-----------+----------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "CPU times: user 590 ms, sys: 432 ms, total: 1.02 s\n",
      "Wall time: 7.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 300 ~ 400 만 : 8 초  \n",
    "# pdf = adf.to_pandas()\n",
    "# adf = pq.read_table(\"parq\")\n",
    "mini_pdf = pdf.iloc[2000000:5000000,:]\n",
    "sdf = spark.createDataFrame(mini_pdf)\n",
    "sdf.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048918fb-377c-49f8-95b4-4ee2160b24d7",
   "metadata": {},
   "source": [
    "# sdf 를 pandas 로 전환  \n",
    "간단한 pandas dataframe을 만들어서 spark dataframe 으로 변환해 본다.  \n",
    "pyarrow 버전에 따라 실패할 가능성이 있기 때문에, 간단한 코드로 테스트.  \n",
    "3.6.9 , pyarrow 6에서는 에러 발생 확인  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "183ec299-f081-4423-bb58-42655f9e93c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setop</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  setop  val\n",
       "0     A    1\n",
       "1     B    2\n",
       "2     C    3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# long 컬럼 제외하고 pd로 만들어서 다시 테스트  \n",
    "pdf = pd.DataFrame()\n",
    "pdf[\"setop\"] = [\"A\", \"B\", \"C\"]\n",
    "pdf[\"val\"] = [1, 2, 3]\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1794d81-133f-4e06-b2c0-1621b9f31972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|setop|val|\n",
      "+-----+---+\n",
      "|    A|  1|\n",
      "|    B|  2|\n",
      "|    C|  3|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, IntegerType\n",
    "columns = [\n",
    "    StructField(\"setop\", StringType())\n",
    "    , StructField(\"val\", IntegerType())]\n",
    "sample_schema = StructType(columns)\n",
    "\n",
    "sdf = spark.createDataFrame(pdf, sample_schema)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e10303-2f1e-49ee-9fb9-57953254c257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- setop: string (nullable = true)\n",
      " |-- val: integer (nullable = true)\n",
      "\n",
      "+-----+---+\n",
      "|setop|val|\n",
      "+-----+---+\n",
      "|A    |1  |\n",
      "|B    |2  |\n",
      "|C    |3  |\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setop</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  setop  val\n",
       "0     A    1\n",
       "1     B    2\n",
       "2     C    3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.printSchema()\n",
    "sdf.show(truncate=False)\n",
    "# 아래 명령은 에러 발생한다. spark 3.1.2 arrow 6.0.1  \n",
    "# spark 3.2.1 arrow 6.0.1 에서는 정상 작동  \n",
    "sdf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939bfd2c-a261-47fc-9264-d150926f67dd",
   "metadata": {},
   "source": [
    "대용량 데이터에 대한 전환 테스트  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db590d4d-7ae0-4ff0-b27f-1dfee4506cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.66 ms, sys: 0 ns, total: 5.66 ms\n",
      "Wall time: 327 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[47] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# spark.rpc.message.maxSize , maxResultSize 늘려도 400만 건 이상에서는 커럽트 발생  \n",
    "# 400만 23 초  \n",
    "tbl_setop_name = 'inven/table-set-6m-20-1000'\n",
    "file_format = 'parquet'\n",
    "\n",
    "sdf = spark.read.format(file_format).load(tbl_setop_name).createOrReplaceTempView(\"tmp\")\n",
    "sdf_tmp = spark.sql(\"select * from tmp limit 3000000\")\n",
    "# sdf_tmp.rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda9e0f-f9b4-4872-816c-1738ef343bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pdf = sdf_tmp.toPandas()\n",
    "print(pdf.shape) \n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f2a57d4-dc5f-4d3a-a223-58ad7387177f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-84d955be8e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdf_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdf' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "del pdf, sdf_tmp, sdf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14979be5-1780-4782-af5d-8b092d105715",
   "metadata": {},
   "source": [
    "# 직접 전환시 에러 발생해서 우회책  \n",
    "- spark data를 hive에 파일 기록  \n",
    "- arrow로 hdfs parq 읽어들이기  \n",
    "- adf 를 pdf 로 전환  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ba19f-80f1-43bc-b020-297191584362",
   "metadata": {},
   "source": [
    "---  \n",
    "## arrow로 hdfs parq 읽기\n",
    "또는 sdf를 adf로 전환  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9ceb0ba-b022-409b-9bec-f5fcbdcd7027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.93 ms, sys: 0 ns, total: 3.93 ms\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#from pyarrow import fs\n",
    "#fs.HadoopFileSystem(\"hdfs://namenode:8020?user=hdfsuser\")\n",
    "# hdfs = fs.HadoopFileSystem(host=\"default\")\n",
    "# hdfs\n",
    "# hdfs.create_dir(\"this_is_where\")\n",
    "# print(hdfs.get_file_info('this_is_where'))\n",
    "# # fs = pyarrow.fs.HadoopFileSystem(\"hdfs:///\")\n",
    "# df = fs.read_parquet('inven/table-set-6m-20-1000') #, **other_options)\n",
    "# print('file : ' + str(df.shape))\n",
    "# df.head()\n",
    "tbl_setop_name = 'inven/table-set-6m-20-1000'\n",
    "file_format = 'parquet'\n",
    "\n",
    "sdf = spark.read.format(file_format).load(tbl_setop_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3a5dc67-9d3c-430d-91bb-0685d175275a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import pyarrow\n",
    "\n",
    "# # hdfs 연결 테스트  : RPC 연결 에러 발생 상태 !!!!!!!! \n",
    "# # rpc port 9000 을 spark edge(master)와 열고 다시 테스트  \n",
    "# # spark write 시에는 어떻게 hdfs로 넘어가는 지 작동 과정 확인할 것 !!!  \n",
    "# client = pyarrow.hdfs.HadoopFileSystem(\"hdfs://namenode:9000\")\n",
    "# client.ls(\"/user/root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2368d766-f664-49c3-ace7-27c845738d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2836494-ba67-4557-ab52-4f41399f765c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a2e587-ffee-496e-a72e-a404db85eaa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5c6b4d5d9db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
