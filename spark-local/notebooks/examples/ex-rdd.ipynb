{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff8b25d-30fe-4efc-8b47-14d81083a0b8",
   "metadata": {},
   "source": [
    "# RDD 샘플 코드  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476ec24-2db7-43f4-981c-bb04b30f6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test용 hdfs 데이터 생성  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa3bfe-8185-48e3-8c75-287a701fff80",
   "metadata": {},
   "source": [
    "## Spark context 생성  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784741b6-456b-49d3-8c7a-92caf9093fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 임포트  \n",
    "import socket\n",
    "import sys\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from os.path import abspath\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "858a2c2e-ea39-428d-9570-c5c6c778834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 정의  \n",
    "scale = 50 \n",
    "partition_num = 2\n",
    "tbl_name = 'examples/rdd-data'\n",
    "file_format = 'parquet'\n",
    "\n",
    "PRJ_ROOT = '/user/root'\n",
    "APP_NAME = 'EXAMPLES-RDD'\n",
    "DB_NAME = 'examples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222be90d-c951-4271-9bb3-86f51bc61077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 생성 \n",
    "def spark_creation():\n",
    "    spark = SparkSession.builder.master('yarn').appName(APP_NAME)\\\n",
    "    .config('spark.rpc.message.maxSize', '1024')\\\n",
    "    .config('spark.hadoop.dfs.replication', '1')\\\n",
    "    .config('spark.driver.cores', '1').config('spark.driver.memory', '7g')\\\n",
    "    .config('spark.num.executors', '3')\\\n",
    "    .config('spark.executor.cores', '1').config('spark.executor.memory', '7g').getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    sc\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f39c3a7-b8b5-43c8-b1ba-140cdc635f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>EXAMPLES-RDD</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2582be3cf8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = spark_creation()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49f83422-7d64-4aeb-b8c6-3716c0f5e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터 생성 \n",
    "def create_samples():\n",
    "    setop_count = scale \n",
    "    inv_rate = 100/setop_count\n",
    "    # 1개월에 최대 1000 건 청약 가정. \n",
    "    inv_val = 1000\n",
    "    inv_req = 0\n",
    "    setop_name = ['ST_A', 'ST_B', 'ST_C', 'ST_D', 'ST_E', 'ST_F', 'ST_G', 'ST_H', 'ST_I', 'ST_J']\n",
    "    setops = []\n",
    "    for s in setop_name:\n",
    "        for i in range(0, int(setop_count/len(setop_name))):\n",
    "            setop_id = f'{s}_{i:07d}'\n",
    "            setops.append([ setop_id, s, inv_rate, inv_val, inv_req ])\n",
    "    return setops\n",
    "\n",
    "# 샘플 데이터 형식 정의. 읽기/쓰기 편의 제공. \n",
    "def define_schema():\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n",
    "    columns = [\n",
    "        StructField(\"setop\", StringType())\n",
    "        , StructField(\"stype\", StringType())\n",
    "        , StructField(\"inv_rate_01\", FloatType())\n",
    "        , StructField(\"inv_val_01\", LongType())\n",
    "        , StructField(\"inv_req_01\", LongType())\n",
    "    ]\n",
    "    sample_schema = StructType(columns)\n",
    "    return sample_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44d5fa27-3f4d-40e4-b292-8831ed33f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----------+----------+----------+\n",
      "|       setop|stype|inv_rate_01|inv_val_01|inv_req_01|\n",
      "+------------+-----+-----------+----------+----------+\n",
      "|ST_A_0000000| ST_A|        2.0|      1000|         0|\n",
      "|ST_A_0000001| ST_A|        2.0|      1000|         0|\n",
      "|ST_A_0000002| ST_A|        2.0|      1000|         0|\n",
      "|ST_A_0000003| ST_A|        2.0|      1000|         0|\n",
      "|ST_A_0000004| ST_A|        2.0|      1000|         0|\n",
      "|ST_B_0000000| ST_B|        2.0|      1000|         0|\n",
      "|ST_B_0000001| ST_B|        2.0|      1000|         0|\n",
      "|ST_B_0000002| ST_B|        2.0|      1000|         0|\n",
      "|ST_B_0000003| ST_B|        2.0|      1000|         0|\n",
      "|ST_B_0000004| ST_B|        2.0|      1000|         0|\n",
      "+------------+-----+-----------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 11.3 ms, sys: 2 ms, total: 13.3 ms\n",
      "Wall time: 151 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# 샘플 데이터 생성 및 확인 \n",
    "sample_data = create_samples()\n",
    "sample_schema = define_schema()\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(sample_data, partition_num)\n",
    "df = spark.createDataFrame(rdd, sample_schema)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7b5d2c2-604b-46bc-9254-2d61ed3d4b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.1 ms, sys: 0 ns, total: 3.1 ms\n",
      "Wall time: 668 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "write_mode = 'overwrite'\n",
    "df.write.save(path=tbl_name, format=file_format, mode=write_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee144f-53e6-4227-b26a-8b25f199a034",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "278a72ce-10f0-4702-a2b5-c9cfa10a8290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[105] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.read.format(file_format).load(tbl_name)\n",
    "sdf.persist()\n",
    "rdd = sdf.rdd\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "84b624e5-6878-40ea-a741-4094290d7998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[122] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = rdd.map(lambda s: (s['stype'], 1))\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "# counts = pairs.reduce(lambda a, b: a+b)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8776b28e-fee7-4359-9c6b-b382a7d4a8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ST_A', 5),\n",
       " ('ST_C', 5),\n",
       " ('ST_H', 5),\n",
       " ('ST_I', 5),\n",
       " ('ST_J', 5),\n",
       " ('ST_F', 5),\n",
       " ('ST_G', 5),\n",
       " ('ST_B', 5),\n",
       " ('ST_D', 5),\n",
       " ('ST_E', 5)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = counts.collect()\n",
    "# len(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff081694-aa3f-4a3e-8f36-59b3bb632323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cf8399dc-2529-4f9d-ae6d-e217d5021e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "/usr/local/lib/python3.6/dist-packages/pyspark/pandas/frame.py:4814: FutureWarning: Deprecated in 3.2, Use DataFrame.spark.to_spark_io instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, Use DataFrame.spark.to_spark_io instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 226 ms, sys: 40.7 ms, total: 267 ms\n",
      "Wall time: 2.12 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setop</th>\n",
       "      <th>stype</th>\n",
       "      <th>inv_rate_01</th>\n",
       "      <th>inv_val_01</th>\n",
       "      <th>inv_req_01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ST_A_0000000</td>\n",
       "      <td>ST_A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ST_A_0000001</td>\n",
       "      <td>ST_A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ST_A_0000002</td>\n",
       "      <td>ST_A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ST_A_0000003</td>\n",
       "      <td>ST_A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ST_A_0000004</td>\n",
       "      <td>ST_A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ST_B_0000000</td>\n",
       "      <td>ST_B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ST_B_0000001</td>\n",
       "      <td>ST_B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ST_B_0000002</td>\n",
       "      <td>ST_B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ST_B_0000003</td>\n",
       "      <td>ST_B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ST_B_0000004</td>\n",
       "      <td>ST_B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          setop stype  inv_rate_01  inv_val_01  inv_req_01\n",
       "0  ST_A_0000000  ST_A          2.0        1000           0\n",
       "1  ST_A_0000001  ST_A          2.0        1000           0\n",
       "2  ST_A_0000002  ST_A          2.0        1000           0\n",
       "3  ST_A_0000003  ST_A          2.0        1000           0\n",
       "4  ST_A_0000004  ST_A          2.0        1000           0\n",
       "5  ST_B_0000000  ST_B          2.0        1000           0\n",
       "6  ST_B_0000001  ST_B          2.0        1000           0\n",
       "7  ST_B_0000002  ST_B          2.0        1000           0\n",
       "8  ST_B_0000003  ST_B          2.0        1000           0\n",
       "9  ST_B_0000004  ST_B          2.0        1000           0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "# spark pandas 이용하기 \n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# psdf = ps.from_pandas(pdf)  \n",
    "psdf = sdf.to_pandas_on_spark()\n",
    "psdf.to_spark_io('zoo.parq', format=\"parquet\")\n",
    "ps.read_spark_io('zoo.parq', format=\"parquet\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6184045-4b85-4e9d-ba4d-68ce95c9447a",
   "metadata": {},
   "source": [
    "# 정지  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "20933391-af61-40d3-8f76-75079062a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
