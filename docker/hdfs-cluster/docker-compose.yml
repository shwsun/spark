version: '3'
services:

  namenode:
    image: shwsun/hive-spark
    container_name: namenode
    hostname: namenode
    volumes:
      - /hdfs/namenode:/hdfs/node
    environment:
      - CLUSTER_NAME=hive
    ports:
      - "50070:50070"
      - "8088:8088"
      - "9000:9000"
      - "9870:9870"
      - "8050:8050"
      - "19888:19888"
    mem_limit: 4g
    tty: true

  dn01:
    image: shwsun/hive-spark
    container_name: dn01
    hostname: dn01
    volumes:
      - /hdfs/dn01:/hdfs/node
    depends_on:
      - namenode
    ports:
      - "8042:8042"
      - "9083:9083"
      - "10001:10001"
      # metastore thrift http  
    #   - "50075:50075"
    mem_limit: 4g
    tty: true

  dn02:
    image: shwsun/hive-spark
    container_name: dn02
    hostname: dn02
    volumes:
      - /hdfs/dn02:/hdfs/node
    depends_on:
      - namenode
    ports:
      - "8043:8042"
    #   - "50075:50075"
    mem_limit: 4g
    tty: true

  dn03:
    image: shwsun/hive-spark
    container_name: dn03
    hostname: dn03
    volumes:
      - /hdfs/dn03:/hdfs/node
    depends_on:
      - namenode
    ports:
      - "8044:8042"
    #   - "50075:50075"
    mem_limit: 4g
    tty: true

  rdb:
    image: shwsun/mysql-hue
    container_name: rdb
    hostname: rdb
    # must be /data/mysql folder in host machine
    # volumes:
    #   - /data/mysql:/var/lib/mysql
    # depends_on:
    #   - dn03
    # ports:
    #   - "8044:8042"
    # #   - "50075:50075"
    tty: true

  hue:
    image: shwsun/hue
    container_name: hue
    hostname: hue
    #restart: on-failure
    command: bash
    depends_on:
      - "rdb"
    ports:
      - "8890:8888"
    tty: true

  spark-master:
    image: shwsun/hive-spark
    container_name: spark-master
    hostname: spark-master
    #command: bash 
    # run-spark.sh를 실행하려면 node 간에 ssh 연결이 끝난 상태이어야 한다.  
    # 외부 쉘 restart-all.sh 에서 ssh sync를 처리한다. 따라서, ssh sync 완료 후에 실행하도록 변경. 
    #command: bash -c "./run-jupyter.sh;./run-spark.sh;/bin/bash"
    command: bash -c "./run-jupyter.sh;/bin/bash"
    volumes:
      - /spark-git:/notebooks
#      - /spark-git/spark/spark-local/notebooks:/notebooks/spark-local
    depends_on:
      - namenode
    ports:
      - "8080:8080"
      - "8081:8081"
      - "8082:8082"
      - "8889:8888"
      - "8998:8998"
      - "4040-4049:4040-4049"
      - "18080:18080"
    mem_limit: 4g
    tty: true

networks:
    default:
        name: hdfs-cluster-net
